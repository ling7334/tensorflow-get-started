{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深入MNIST\n",
    "TensorFlow是一个非常强大的用来做大规模数值计算的库。其所擅长的任务之一就是实现以及训练深度神经网络。\n",
    "\n",
    "在本教程中，我们将学到构建一个TensorFlow模型的基本步骤，并将通过这些步骤为MNIST构建一个深度卷积神经网络。\n",
    "\n",
    "这个教程假设你已经熟悉神经网络和MNIST数据集。如果你尚未了解，请查看[新手指南](./MNIST_For_ML_Beginners.ipynb)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关于本教程\n",
    "本教程首先解释了[mnist_softmax.py](./mnist_softmax.py)中的代码 —— 一个简单的Tensorflow模型的应用。然后展示了一些提高精度的方法。\n",
    "\n",
    "你可以运行本教程中的代码，或通读代码。\n",
    "\n",
    "本教程将会完成：  \n",
    "创建一个softmax回归算法用以输入MNIST图片来辨识数位的模型，用Tensorfow通过辨识成百上千的例子来训练模型（运行我们首个Tensorflow session）  \n",
    "使用测试数据来测试模型准确度  \n",
    "构建，训练，测试一个多层卷积神经网络来提高精度  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装\n",
    "在创建模型之前，我们会先加载MNIST数据集，然后启动一个TensorFlow的session。\n",
    "\n",
    "### 加载MNIST数据\n",
    "\n",
    "为了方便起见，我们已经准备了一个[脚本](./input_data.py)来自动下载和导入MNIST数据集。它会自动创建一个**MNIST_data**的目录来存储数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，**mnist**是一个轻量级的类。它以Numpy数组的形式存储着训练、校验和测试数据集。同时提供了一个函数，用于在迭代每一小批数据，后面我们将会用到。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 运行TensorFlow的InteractiveSession\n",
    "Tensorflow依赖于一个高效的C++后端来进行计算。与后端的这个连接叫做session。一般而言，使用TensorFlow程序的流程是先创建一个图，然后在session中启动它。\n",
    "\n",
    "这里，我们使用更加方便的**InteractiveSession**类。通过它，你可以更加灵活地构建你的代码。它能让你在运行图的时候，插入一些[计算图](./Getting Started With TensorFlow.ipynb#计算图)，这些计算图是由某些操作(operations)构成的。这对于工作在交互式环境中的人们来说非常便利，比如使用IPython。如果你没有使用**InteractiveSession**，那么你需要在启动session之前构建整个计算图，然后[启动该计算图](./Getting Started With TensorFlow.ipynb#计算图)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算图\n",
    "为了在Python中进行高效的数值计算，我们通常会使用像[NumPy](http://www.numpy.org/)一类的库，将一些诸如矩阵乘法的耗时操作在Python环境的外部来计算，这些计算通常会通过其它语言并用更为高效的代码来实现。\n",
    "\n",
    "但遗憾的是，每一个操作切换回Python环境时仍需要不小的开销。如果你想在GPU或者分布式环境中计算时，这一开销更加可怖，这一开销主要可能是用来进行数据迁移。\n",
    "\n",
    "TensorFlow也是在Python外部完成其主要工作，但是进行了改进以避免这种开销。其并没有采用在Python外部独立运行某个耗时操作的方式，而是先让我们描述一个交互操作图，然后完全将其运行在Python外部。这与Theano或Torch的做法类似。\n",
    "\n",
    "因此Python代码的目的是用来构建这个可以在外部运行的计算图，以及安排计算图的哪一部分应该被运行。详情请查看[基本用法](./Getting Started With TensorFlow.ipynb)中的[计算图](./Getting Started With TensorFlow.ipynb#计算图)一节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Softmax 回归模型\n",
    "在这一节中我们将建立一个拥有一个线性层的softmax回归模型。在下一节，我们会将其扩展为一个拥有多层卷积网络的softmax回归模型。\n",
    "\n",
    "### 占位符\n",
    "我们通过为输入图像和目标输出类别创建节点，来开始构建计算图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的**x**和**y_**并不是特定的值，相反，他们都只是一个**占位符**，可以在TensorFlow运行某一计算时根据该占位符输入具体的值。\n",
    "\n",
    "输入图片**x**是一个2维的浮点数张量。这里，分配给它的**维度**为**[None, 784]**，其中**784**是一张展平的MNIST图片的维度。**None**表示其值大小不定，在这里作为第一个维度值，用以指代batch的大小，意即x的数量不定。输出类别值**y_**也是一个2维张量，其中每一行为一个10维的独热码向量,用于代表对应某一MNIST图片的类别。\n",
    "\n",
    "虽然**占位**符的**维度**参数是可选的，但有了它，TensorFlow能够自动捕捉因数据维度不一致导致的错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 变量\n",
    "我们现在为模型定义权重**W**和偏置**b**。可以将它们当作额外的输入量，但是TensorFlow有一个更好的处理方式：**变量**。一个**变量**代表着TensorFlow计算图中的一个值，能够在计算过程中使用，甚至进行修改。在机器学习的应用过程中，模型参数一般用**变量**来表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在调用**tf.Variable**的时候传入初始值。在这个例子里，我们把**W**和**b**都初始化为零向量。**W**是一个784x10的矩阵（因为我们有784个特征和10个输出值）。**b**是一个10维的向量（因为我们有10个分类）。\n",
    "\n",
    "**变量**需要通过seesion初始化后，才能在session中使用。这一初始化步骤为，为初始值指定具体值（本例当中是全为零），并将其分配给每个**变量**,可以一次性为所有**变量**完成此操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 类别预测与损失函数\n",
    "现在我们可以实现我们的回归模型了。这只需要一行！我们把向量化后的图片**x**和权重矩阵**W**相乘，加上偏置**b**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.matmul(x,W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以指定损失函数来指示模型预测一个实例有多不准；我们要在整个训练过程中使其最小化。这里我们的损失函数是目标类别和预测类别之间的交叉熵。斤现在初级教程中一样，我们使用稳定方程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，**tf.nn.softmax_cross_entropy_with_logits**隐式地对模型未归一化模型预测值和所有类别的总值应用了softmax函数，**tf.reduce_sum**取了总值的平均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型\n",
    "-----\n",
    "我们已经定义好模型和训练用的损失函数，那么用TensorFlow进行训练就很简单了。因为TensorFlow知道整个计算图，它可以使用自动微分法找到对于各个变量的损失的梯度值。TensorFlow有[大量内置的优化算法](https://www.tensorflow.org/api_guides/python/train#optimizers)这个例子中，我们用最速下降法让交叉熵下降，步长为0.5。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一行代码实际上是用来往计算图上添加一个新操作，其中包括计算梯度，计算每个参数的步长变化，并且计算出新的参数值。\n",
    "\n",
    "返回的**train_step**操作对象，在运行时会使用梯度下降来更新参数。因此，整个模型的训练可以通过反复地运行**train_step**来完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一步迭代，我们都会加载50个训练样本，然后执行一次**train_step**，并通过**feed_dict**将**x**和**y_**张量**占位符**用训练替代为训练数据。\n",
    "\n",
    "注意，在计算图中，你可以用**feed_dict**来替代任何张量，并不仅限于替换**占位符**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估模型\n",
    "那么我们的模型性能如何呢？\n",
    "\n",
    "首先让我们找出那些预测正确的标签。**tf.argmax**是一个非常有用的函数，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签，比如**tf.argmax(y,1)**返回的是模型对于任一输入x预测到的标签值，而**tf.argmax(y_,1)**代表正确的标签，我们可以用**tf.equal**来检测我们的预测是否真实标签匹配(索引位置一样表示匹配)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里返回一个布尔数组。为了计算我们分类的准确率，我们将布尔值转换为浮点数来代表对、错，然后取平均值。例如：**[True, False, True, True]**变为**[1,0,1,1]**，计算出平均值为**0.75**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们可以计算出在测试数据上的准确率，大概是92%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9151\n"
     ]
    }
   ],
   "source": [
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建一个多层卷积网络\n",
    "在MNIST上只有91%正确率，实在太糟糕。在这个小节里，我们用一个稍微复杂的模型：卷积神经网络来改善效果。这会达到大概99.2%的准确率。虽然不是最高，但是还是比较让人满意。\n",
    "\n",
    "### 权重初始化\n",
    "为了创建这个模型，我们需要创建大量的权重和偏置项。这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。由于我们使用的是[ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题（dead neurons）。为了不在建立模型的时候反复做初始化操作，我们定义两个函数用于初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积和池化\n",
    "TensorFlow在卷积和池化上有很强的灵活性。我们怎么处理边界？步长应该设多大？在这个实例里，我们会一直使用vanilla版本。我们的卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。我们的池化用简单传统的2x2大小的模板做max pooling。为了代码更简洁，我们把这部分抽象成一个函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一层卷积\n",
    "现在我们可以开始实现第一层了。它由一个卷积接一个max pooling完成。卷积在每个5x5的patch中算出32个特征。卷积的权重张量形状是**[5, 5, 1, 32]**，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。 而对于每一个输出通道都有一个对应的偏置量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了用这一层，我们把**x**变成一个4d向量，其第2、第3维对应图片的宽、高，最后一维代表图片的颜色通道数(因为是灰度图所以这里的通道数为1，如果是rgb彩色图，则为3)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把**x_image**和权值向量进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行max pooling。**max_pool_2x2**方法会将图像降为14乘14。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二层卷积\n",
    "为了构建一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个5x5的patch会得到64个特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 密集连接层\n",
    "现在，图片尺寸减小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。我们把池化层输出的张量reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "为了减少过拟合，我们在输出层之前加入[dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)。我们用一个**placeholder**来代表一个神经元的输出在dropout中保持不变的概率。这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。 TensorFlow的**tf.nn.dropout**操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale。<sup>1</sup>\n",
    "\n",
    "1: 事实上，对于这个小型卷积网络，有没有dropout性能都差不多。dropout通常对降低过拟合总是很有用，但是是对于大型的神经网络来说。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出层\n",
    "=====\n",
    "\n",
    "最后，我们添加一个softmax层，就像前面的单层softmax回归一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练和评估模型\n",
    "-----\n",
    "\n",
    "这个模型的效果如何呢？为了进行训练和评估，我们使用与之前简单的单层SoftMax神经网络模型几乎相同的一套代码，只是我们会用更加复杂的ADAM优化器来做梯度最速下降，在feed_dict中加入额外的参数keep_prob来控制dropout比例。然后每100次迭代输出一次日志。\n",
    "\n",
    "不同之处是：  \n",
    "我们替换最速梯度下降优化器为更复杂的自适应动量估计优化器  \n",
    "我们在**feed_dict**增加了**keep_prob**以控制dropout率  \n",
    "我们在训练过程中每100次迭代记录一次日志  \n",
    "\n",
    "你可以随意运行这段代码，但它有20,000次迭代可能要运行一段时间（可能超过半小时）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.08\n",
      "step 100, training accuracy 0.86\n",
      "step 200, training accuracy 0.86\n",
      "step 300, training accuracy 0.9\n",
      "step 400, training accuracy 0.9\n",
      "step 500, training accuracy 0.96\n",
      "step 600, training accuracy 0.98\n",
      "step 700, training accuracy 0.86\n",
      "step 800, training accuracy 0.98\n",
      "step 900, training accuracy 0.96\n",
      "test 0 accuracy 0.96\n",
      "test 1 accuracy 1\n",
      "test 2 accuracy 0.98\n",
      "test 3 accuracy 0.96\n",
      "test 4 accuracy 0.96\n",
      "test 5 accuracy 0.98\n",
      "test 6 accuracy 0.9\n",
      "test 7 accuracy 0.98\n",
      "test 8 accuracy 1\n",
      "test 9 accuracy 1\n",
      "test 10 accuracy 0.94\n",
      "test 11 accuracy 1\n",
      "test 12 accuracy 0.98\n",
      "test 13 accuracy 0.98\n",
      "test 14 accuracy 0.94\n",
      "test 15 accuracy 0.96\n",
      "test 16 accuracy 0.94\n",
      "test 17 accuracy 0.98\n",
      "test 18 accuracy 1\n",
      "test 19 accuracy 0.98\n",
      "test 20 accuracy 1\n",
      "test 21 accuracy 0.96\n",
      "test 22 accuracy 0.96\n",
      "test 23 accuracy 0.96\n",
      "test 24 accuracy 0.92\n",
      "test 25 accuracy 0.96\n",
      "test 26 accuracy 1\n",
      "test 27 accuracy 0.96\n",
      "test 28 accuracy 0.96\n",
      "test 29 accuracy 0.98\n",
      "test 30 accuracy 0.98\n",
      "test 31 accuracy 0.98\n",
      "test 32 accuracy 0.96\n",
      "test 33 accuracy 0.96\n",
      "test 34 accuracy 1\n",
      "test 35 accuracy 0.98\n",
      "test 36 accuracy 1\n",
      "test 37 accuracy 0.94\n",
      "test 38 accuracy 0.98\n",
      "test 39 accuracy 0.96\n",
      "test 40 accuracy 1\n",
      "test 41 accuracy 0.96\n",
      "test 42 accuracy 0.96\n",
      "test 43 accuracy 1\n",
      "test 44 accuracy 1\n",
      "test 45 accuracy 0.8\n",
      "test 46 accuracy 0.98\n",
      "test 47 accuracy 0.9\n",
      "test 48 accuracy 0.92\n",
      "test 49 accuracy 0.94\n",
      "test 50 accuracy 0.92\n",
      "test 51 accuracy 0.98\n",
      "test 52 accuracy 0.96\n",
      "test 53 accuracy 0.9\n",
      "test 54 accuracy 1\n",
      "test 55 accuracy 0.94\n",
      "test 56 accuracy 0.96\n",
      "test 57 accuracy 0.96\n",
      "test 58 accuracy 0.98\n",
      "test 59 accuracy 0.96\n",
      "test 60 accuracy 0.9\n",
      "test 61 accuracy 1\n",
      "test 62 accuracy 1\n",
      "test 63 accuracy 0.96\n",
      "test 64 accuracy 0.98\n",
      "test 65 accuracy 0.94\n",
      "test 66 accuracy 0.98\n",
      "test 67 accuracy 1\n",
      "test 68 accuracy 0.94\n",
      "test 69 accuracy 0.92\n",
      "test 70 accuracy 0.98\n",
      "test 71 accuracy 0.98\n",
      "test 72 accuracy 0.98\n",
      "test 73 accuracy 0.98\n",
      "test 74 accuracy 0.96\n",
      "test 75 accuracy 0.96\n",
      "test 76 accuracy 1\n",
      "test 77 accuracy 0.92\n",
      "test 78 accuracy 0.98\n",
      "test 79 accuracy 0.98\n",
      "test 80 accuracy 0.98\n",
      "test 81 accuracy 0.92\n",
      "test 82 accuracy 0.94\n",
      "test 83 accuracy 0.98\n",
      "test 84 accuracy 1\n",
      "test 85 accuracy 0.98\n",
      "test 86 accuracy 0.92\n",
      "test 87 accuracy 0.98\n",
      "test 88 accuracy 0.86\n",
      "test 89 accuracy 0.94\n",
      "test 90 accuracy 1\n",
      "test 91 accuracy 0.96\n",
      "test 92 accuracy 0.98\n",
      "test 93 accuracy 0.96\n",
      "test 94 accuracy 0.98\n",
      "test 95 accuracy 0.98\n",
      "test 96 accuracy 0.98\n",
      "test 97 accuracy 0.92\n",
      "test 98 accuracy 0.92\n",
      "test 99 accuracy 0.98\n",
      "test average accuracy 0.9634\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "# print (\"test accuracy %g\" % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
    "# Tensorflow throw OOM error if evaluate accuracy at once and memory is not enough\n",
    "cross_accuracy = 0\n",
    "for i in range(100):\n",
    "    testSet = mnist.test.next_batch(50)\n",
    "    each_accuracy = accuracy.eval(feed_dict={ x: testSet[0], y_: testSet[1], keep_prob: 1.0})\n",
    "    cross_accuracy += each_accuracy\n",
    "    print(\"test %d accuracy %g\" % (i,each_accuracy))\n",
    "print(\"test average accuracy %g\" % (cross_accuracy/100,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码，在最终测试集上的准确率大概是99.2%。\n",
    "\n",
    "目前为止，我们已经学会了用TensorFlow快捷地搭建、训练和评估一个复杂一点儿的深度学习模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
